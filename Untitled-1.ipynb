{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name=\"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts=[\"เก้าอี้สีขาวทันสมัย\",\"โต๊ะยุคใหม่ สวยงาม\",\"โต๊ะสีดำ ��\",\"ดูดีมาก\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True,return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 81912576\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geneated Text #1: ขอคำโฆษณาเกี่ยวกับเก้าอี้งงสาวกนิงสา็สาวกนิงสา็สาว\n",
      "Geneated Text #2: ขอคำโฆษณาเกี่ยวกับเก้าอี้าอีาอี้่ยวกาอีาอีาอีาอีาอ\n",
      "Geneated Text #3: ขอคำโฆษณาเกี่ยวกับเก้าอี้าอีเกี่ยวกับเกี่ยวกับเกี่\n",
      "Geneated Text #4: ขอคำโฆษณาเกี่ยวกับเก้าอี้าอีสกปงรพาศกาิพอี้าอี้าอี\n",
      "Geneated Text #5: ขอคำโฆษณาเกี่ยวกับเก้าอี้าอี้าอี้าอี้าอี้าอี้าอี้า\n",
      "Geneated Text #6: ขอคำโฆษณาเกี่ยวกับเก้าอี้ืกสมกรอวกงงับกะาอี้ิยวกง.\n",
      "\n",
      "Geneated Text #7: ขอคำโฆษณาเกี่ยวกับเก้าอี้ยอนพเธุกพสริ้ยอนพพเกับบวก\n",
      "Geneated Text #8: ขอคำโฆษณาเกี่ยวกับเก้าอี้า จสารปมอะหกอ้าลซบลงาิะรย�\n",
      "Geneated Text #9: ขอคำโฆษณาเกี่ยวกับเก้าอีู้รจยวกอี้าอีิามามามาอีิาม\n",
      "Geneated Text #10: ขอคำโฆษณาเกี่ยวกับเก้าอี้าอง่ยวกับเกี่ยวกับเกี่ยวก\n"
     ]
    }
   ],
   "source": [
    "prompt_text=\"ขอคำโฆษณาเกี่ยวกับเก้าอี้\"\n",
    "max_length=100\n",
    "num_samples=10\n",
    "\n",
    "generated_texts=[]\n",
    "for _ in range(num_samples):\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids,max_length=max_length,pad_token_id=tokenizer.eos_token_id,num_return_sequences=1,do_sample=True,temperature=0.8)\n",
    "    generated_texts.append(tokenizer.decode(outputs[0],skip_special_tokens=True))\n",
    "for i,text in enumerate(generated_texts):\n",
    "    print(f\"Geneated Text #{i+1}: {text}\"   )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m loss_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_encodings, labels\u001b[38;5;241m=\u001b[39mtrain_encodings\u001b[38;5;241m.\u001b[39minput_ids)\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1077\u001b[0m     input_ids,\n\u001b[0;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:843\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    840\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[0;32m    842\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[0;32m    844\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[0;32m    845\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\first\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "train_texts=[\"เก้าอี้สีขาวทันสมัย\",\"โต๊ะยุคใหม่ สวยงาม\",\"โต๊ะสีดำ ��\",\"ดูดีมาก\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "optmizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "loss_default=10\n",
    "for epoch in range(1):\n",
    "    outputs = model(**train_encodings, labels=train_encodings.input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optmizer.step()\n",
    "    optmizer.zero_grad()\n",
    "    print(f\"Loss after epoch {epoch+1}: {loss.item()}\")\n",
    "    if loss.item()<loss_default:\n",
    "        loss_default=loss.item()\n",
    "        output_dir = \"./model_save\"\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geneated Text #1: โฆษษาเกี่ยวกับเก้าอี้นยสีวยี้ม มัสยยชีสหม สยยนสม \n",
      "Geneated Text #2: โฆษษาเกี่ยวกับเก้าอี้นต้ายามสัาวยมัยยวยยยสง с ��\n",
      "Geneated Text #3: โฆษษาเกี่ยวกับเก้าอี้ยงสยสยางกวหยวสัยมี้อนยดยอมสาห\n",
      "Geneated Text #4: โฆษษาเกี่ยวกับเก้าอีู้ยามีวสสยสยายดยนัยหาม ยืู ใสวั\n",
      "Geneated Text #5: โฆษษาเกี่ยวกับเก้าอี้ะวยสันยสาหาสสสมวยยา\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    " \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-n', '--name', default=\"เก้าอี้\",type=str, help='Product')\n",
    " \n",
    "args = parser.parse_args()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "output_dir=\"./model_save\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(\"cuda\")\n",
    "#prompt_text=\"โฆษษาเกี่ยวกับ\"\n",
    "prompt_text=f\"{args.name}\"\n",
    "max_length=500\n",
    "num_samples=5\n",
    "#want to use output as input\n",
    "generated_texts=[]\n",
    "for _ in range(num_samples):\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids,max_length=max_length,pad_token_id=tokenizer.eos_token_id,num_return_sequences=1,do_sample=True,temperature=0.8)\n",
    "    text_output=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    generated_texts.append(text_output)\n",
    "    prompt_text=text_output\n",
    "for text in generated_texts:\n",
    "    print(text,end=\"\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'dataset'\n",
      "d:\\KMITL\\KeyToAd\\dataset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%cd dataset\n",
    "df1=pd.read_csv(\"alldataikea.csv\")\n",
    "df1 = df1[['advert']]\n",
    "df1 = df1.rename(columns={'advert': 'text'})\n",
    "df2=pd.read_csv(\"Bedroom.csv\")\n",
    "df2 = df2[['Field2']]\n",
    "df2 = df2.rename(columns={'Field2': 'text'})\n",
    "df3=pd.read_csv(\"Category _ Mood and Tone Furniture _ Bangkok Online Furniture _ Custom Furniture _ รับผลิตเฟอร์นิเจอ.csv\")\n",
    "df3 = df3[['Field2']]\n",
    "df3 = df3.rename(columns={'Field2': 'text'})\n",
    "df4=pd.read_csv(\"storage.csv\")\n",
    "df4 = df4[['Field2']]\n",
    "df4= df4.rename(columns={'Field2': 'text'})\n",
    "df5=pd.read_csv(\"เฟอร์นิเจอร์ — Thailandoutdoorshop.csv\")\n",
    "df5 = df5[['Field1']]\n",
    "df5= df5.rename(columns={'Field1': 'text'})\n",
    "df6=pd.read_csv(\"วันนี้มีดีลเด็ด รอคุณอยู่ ว.csv\")\n",
    "df6 = df6[['Field2']]\n",
    "df6= df6.rename(columns={'Field2': 'text'})\n",
    "df7=pd.read_csv(\"สินค้า - PROMA.csv\")\n",
    "df7= df7[['Field2']]\n",
    "df7= df7.rename(columns={'Field2': 'text'})\n",
    "df8 =pd.read_csv(\"สินค้าทั้งหมด - HomeStudio.csv\")\n",
    "df8=df8[['Field1']]\n",
    "df8= df8.rename(columns={'Field1': 'text'})\n",
    "df9 =pd.read_csv(\"สินค้าทั้งหมด - HomeStudio(1).csv\")\n",
    "df9=df9[['Field1']]\n",
    "df9= df9.rename(columns={'Field1': 'text'})\n",
    "#merge all data to one dataframe\n",
    "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9],ignore_index=True)\n",
    "df=df.dropna()\n",
    "df=df.drop_duplicates()\n",
    "#df=df.sample(frac=1).reset_index(drop=True)\n",
    "df.to_csv(\"all_data.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              output input  \\\n",
      "0  มีป้ายกำกับหน้าลิ้นชักแต่ละช่อง เพื่อความเรียบ...         \n",
      "1  เป็นทุกอย่างที่คุณต้องการสำหรับเก็บของให้เป็นร...         \n",
      "2  ตู้ลิ้นชัก NORDLI/นูร์ดลี ที่คุณออกแบบได้ตามต้...         \n",
      "3             ล้อเลื่อนช่วยให้เคลื่อนย้ายตู้ได้สะดวก         \n",
      "4  รับประกันนาน 10 ปี อ่านเงื่อนไขการรับประกันได้...         \n",
      "\n",
      "                             instruction  \\\n",
      "0  สร้างประโยคโฆษณาเกี่ยวกับเฟอร์นิเจอร์   \n",
      "1  สร้างประโยคโฆษณาเกี่ยวกับเฟอร์นิเจอร์   \n",
      "2  สร้างประโยคโฆษณาเกี่ยวกับเฟอร์นิเจอร์   \n",
      "3  สร้างประโยคโฆษณาเกี่ยวกับเฟอร์นิเจอร์   \n",
      "4  สร้างประโยคโฆษณาเกี่ยวกับเฟอร์นิเจอร์   \n",
      "\n",
      "                                                text  \n",
      "0  Below is an instruction that describes a task,...  \n",
      "1  Below is an instruction that describes a task,...  \n",
      "2  Below is an instruction that describes a task,...  \n",
      "3  Below is an instruction that describes a task,...  \n",
      "4  Below is an instruction that describes a task,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"all_data.csv\")\n",
    "df= df.rename(columns={'text': 'output'})\n",
    "#crate input column with \"\"\n",
    "df['input'] = \"\"\n",
    "df['instruction'] = \"สร้างประโยคโฆษณาเกี่ยวกับเฟอร์นิเจอร์\"\n",
    "text_col=[]\n",
    "for _,row in df.iterrows():\n",
    "    prompt =\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
    "    instruction = str(row['instruction'])\n",
    "    input_query = str(row['input'])\n",
    "    response = str(row['output'])\n",
    "    text = f\"{prompt} ### Instruction:\\n{instruction} \\n###Response:\\n {response}\"\n",
    "    \n",
    "    text_col.append(text)\n",
    "df.loc[:,'text']=text_col\n",
    "print(df.head())\n",
    "df.to_csv(\"../train.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3a618fee3f5630b3ed868cad6c93717ab4bd22edd9fe5b33dcddd4fbe7ab104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
